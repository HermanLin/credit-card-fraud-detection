{
 "cells": [
  {
   "source": [
    "# Credit Card Fraud Detection Project\n",
    "*By: Herman Lin and Mahika Jain*\n",
    "---\n",
    "blah"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mporting the libraries to be used:\n",
    "import sklearn\n",
    "from sklearn import preprocessing, svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.polynomial.polynomial as poly\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(284807, 31)\n"
     ]
    }
   ],
   "source": [
    "# Read .csv file and put data into a pandas dataframe\n",
    "df = pd.read_csv('archive.zip')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(284807, 31)\n"
     ]
    }
   ],
   "source": [
    "# Drop empty columns\n",
    "df1 = df.dropna('columns')\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(284807, 31)\n"
     ]
    }
   ],
   "source": [
    "# Convert dataframe into a numpy array\n",
    "df2 = np.array(df1)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Feature Names: ['Time' 'V1' 'V2' 'V3' 'V4' 'V5' 'V6' 'V7' 'V8' 'V9' 'V10' 'V11' 'V12'\n 'V13' 'V14' 'V15' 'V16' 'V17' 'V18' 'V19' 'V20' 'V21' 'V22' 'V23' 'V24'\n 'V25' 'V26' 'V27' 'V28' 'Amount']\n"
     ]
    }
   ],
   "source": [
    "# Printing the names of all the features\n",
    "# - Note: Most feature names have been anonymized to preserve confidentiality\n",
    "features = np.array(df.columns[:30])\n",
    "print('Feature Names:', features)"
   ]
  },
  {
   "source": [
    "### Note:\n",
    "\n",
    "The credit card dataset we are using for this project is naturally unbalanced. There are significantly more examples that are classified as non-fradulent than there are fradulent. One way to help counteract this is to undersample the majority class and oversample the minority class. Thus, we will be scaling our training set to contain a ratio of 5:1 non-fradulent to fradulent as well as using a fraction of the original dataset as our training and validation sets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the data into class_0 and class_1 examples\n",
    "zero = []\n",
    "one = []\n",
    "num_examples = df2.shape[0]\n",
    "\n",
    "for i in range(num_examples):\n",
    "    if df2[i][30] == 0:\n",
    "        zero.append(df2[i])\n",
    "    else: \n",
    "        one.append(df2[i])\n",
    "\n",
    "class_0 = np.array(zero)\n",
    "class_1 = np.array(one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of Class 0: 284315\nNumber of Class 1: 492\n"
     ]
    }
   ],
   "source": [
    "# Verify shapes of class_0 and class_1\n",
    "print('Number of Class 0:', class_0.shape[0])\n",
    "print('Number of Class 1:', class_1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly choose 2460 examples from the entire class_0 set\n",
    "class_0_reduced = class_0[np.random.choice(284315, 2460, replace=False),:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2952, 31)\n"
     ]
    }
   ],
   "source": [
    "# Combine samples together and randomize the samples\n",
    "reduced_data = np.concatenate((class_0_reduced, class_1))\n",
    "np.random.shuffle(reduced_data)\n",
    "print(reduced_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate columns into features and target\n",
    "X = np.array(reduced_data[:,0:30]) # all rows, first 30 columns\n",
    "y = np.array(reduced_data[:,30]) # all rows, last column"
   ]
  },
  {
   "source": [
    "# sklearn Model Implementation\n",
    "\n",
    "We have created a function for easy model testing of the data. By specifying certain parameters, we are able to run either a Logisitic Regression Model (with different regularization methods) or an SVM Model (with different kernels)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#           ====================\n",
    "#           TEST DEGREES LATER!! (for polynomial kernels)\n",
    "#           ====================\n",
    "#\n",
    "\n",
    "def sklearn_model(X_tr, y_tr, X_ts, y_ts, m_type, c, iters, penalty='none', kernel=None):\n",
    "    acc_tr_model = []\n",
    "    acc_ts_model = []\n",
    "    c_model = []\n",
    "    model = None\n",
    "\n",
    "    # create model \n",
    "    if m_type == 0:\n",
    "        model = LogisticRegression(penalty=penalty, C=c, solver='saga',max_iter=iters)\n",
    "        print('Training Logistic Regression Model...')\n",
    "    elif m_type == 1:\n",
    "        model = svm.SVC(kernel=kernel, C=c)\n",
    "        print('Training SVM Model...')\n",
    "    \n",
    "    # fit the model\n",
    "    model.fit(X_tr, y_tr)\n",
    "\n",
    "    # find the prediction on the training and testing set\n",
    "    yhat_tr = model.predict_proba(X_tr)\n",
    "    yhat_ts = model.predict_proba(X_ts)\n",
    "\n",
    "    # calculate and add accuracy values to respective lists\n",
    "    acc_tr = model.score(X_tr, y_tr)\n",
    "    acc_tr_model.append(acc_tr)\n",
    "    print(\"Accuracy on training data = %f\" % acc_tr)\n",
    "    acc_ts = model.score(X_ts, y_ts)\n",
    "    acc_ts_model.append(acc_ts)\n",
    "    print(\"Accuracy on test data = %f\" % acc_ts)\n",
    "\n",
    "    # appending value of c for graphing purposes if needed\n",
    "    c_model.append(c)\n",
    "\n",
    "    # creating a confusion matrix for analysis\n",
    "    confuse_matrix_tr = confusion_matrix(y_tr, model.predict(X_tr))\n",
    "    class_report_tr = classification_report(y_tr, model.predict(X_tr))\n",
    "    confuse_matrix_ts = confusion_matrix(y_ts, model.predict(X_ts))\n",
    "    class_report_ts = classification_report(y_ts, model.predict(X_ts))\n",
    "\n",
    "    return acc_tr_model, acc_ts_model, c_model, model, (confuse_matrix_tr, confuse_matrix_ts), (class_report_tr, class_report_ts)"
   ]
  },
  {
   "source": [
    "# Logisitic Regression\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Step 1: Data Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data by preprocessing\n",
    "# - The idea behind StandardScaler is that it will transform your data such that \n",
    "#   its distribution will have a mean value 0 and standard deviation of 1.\n",
    "# - Mean Subtraction: for every feature subtract the mean\n",
    "#   Normalization: make all features roughly the same size\n",
    "#       Xâ€™ = (x-mean)/std\n",
    "\n",
    "X_scale = preprocessing.StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the reduced_data into the training and testing sets\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X_scale, y, test_size=0.3, random_state=133)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 1.18356000e+05 -9.04911725e-01 -9.39549794e-01 ... -8.91547618e-02\n   4.10001376e-02  2.67950000e+02]\n [ 1.72900000e+04  8.90523708e-01 -7.59219675e-01 ...  6.06610928e-02\n   5.40860299e-02  1.65600000e+02]\n [ 3.81870000e+04 -1.12944870e+00  6.40992600e-01 ... -4.72280137e-02\n   3.00811990e-02  3.56400000e+01]\n ...\n [ 4.22260000e+04 -2.11690519e-01  5.56751933e-01 ...  1.58927648e-01\n   2.22483930e-01  2.77300000e+01]\n [ 7.30180000e+04  1.15502006e+00 -8.77870787e-02 ...  5.80874894e-02\n   1.96268764e-02  1.00000000e+00]\n [ 1.42865000e+05 -3.15565336e-01  6.04198629e-01 ...  2.50562693e-02\n   3.97947366e-02  8.28000000e+00]]\n\n=======================After StandardScalar==========================\n[[ 5.84134246e-01 -3.59965785e-02 -5.83708612e-01 ... -1.97913344e-01\n   9.52579574e-02  6.35859474e-01]\n [-1.55916916e+00  4.46834574e-01 -5.15521857e-01 ...  3.51293799e-02\n   1.36948772e-01  2.53335806e-01]\n [-1.11600715e+00 -9.63793932e-02  1.39289751e-02 ... -1.32695123e-01\n   6.04709181e-02 -2.32377685e-01]\n ...\n [-1.03035221e+00  1.50425494e-01 -1.79242603e-02 ...  1.87985736e-01\n   6.73453700e-01 -2.61940579e-01]\n [-3.77347259e-01  5.17963323e-01 -2.61638533e-01 ...  3.11260683e-02\n   2.71641159e-02 -3.61841485e-01]\n [ 1.10389582e+00  1.22491324e-01  1.63714848e-05 ... -2.02549122e-02\n   9.14176269e-02 -3.34633157e-01]]\n"
     ]
    }
   ],
   "source": [
    "print('\\n======================Before StandardScalar==========================')\r\n",
    "print(X)\r\n",
    "print('\\n=======================After StandardScalar==========================')\r\n",
    "print(X_scale)"
   ]
  },
  {
   "source": [
    "### Step 2: Create and run the logistic regression model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Logistic Regression Model...\n",
      "C:\\Users\\Hermano\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1322: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "Accuracy on training data = 0.982575\n",
      "Accuracy on test data = 0.974041\n"
     ]
    }
   ],
   "source": [
    "# Perform Logisitic Regression with no Regularization\n",
    "acc_tr_logreg, acc_ts_logreg, c_logreg, logreg_model, confusion_matrices, class_reports = sklearn_model(X_train, y_train, X_test, y_test, 0, 100000000, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(array([[1712,    4],\n       [  32,  318]], dtype=int64), array([[739,   5],\n       [ 18, 124]], dtype=int64))\n\n              precision    recall  f1-score   support\n\n         0.0       0.98      1.00      0.99      1716\n         1.0       0.99      0.91      0.95       350\n\n    accuracy                           0.98      2066\n   macro avg       0.98      0.95      0.97      2066\nweighted avg       0.98      0.98      0.98      2066\n\n\n              precision    recall  f1-score   support\n\n         0.0       0.98      0.99      0.98       744\n         1.0       0.96      0.87      0.92       142\n\n    accuracy                           0.97       886\n   macro avg       0.97      0.93      0.95       886\nweighted avg       0.97      0.97      0.97       886\n\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrices)\n",
    "print()\n",
    "print(class_reports[0])\n",
    "print()\n",
    "print(class_reports[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0a868f5554589e2af52e9ac65f4e9cf67d07107fa900cd1827df20b8357fa2962",
   "display_name": "Python 3.8.5 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}